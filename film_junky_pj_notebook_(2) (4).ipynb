{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c1436fbc",
      "metadata": {
        "id": "c1436fbc"
      },
      "source": [
        "# Film Junky Union Review Analysis using ML:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7237b9db",
      "metadata": {
        "id": "7237b9db"
      },
      "source": [
        "### NOTE: ###\n",
        "**I could not install the extension for Intel's GPU for Torch and as such could not run the BERT portion of my project on my local PC instead I had to use Google Colab for access to a GPU compatible with torch 'cuda'. This current notebook is only a copy of my colab notebook.**\n",
        "\n",
        "**Here's a link to the Colab Notebook I worked on that has all the outputs from my code**\n",
        "https://colab.research.google.com/drive/1ieHZcyKJB5AFtj6L6L5kYCrV_rRTTDTm?usp=sharing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "663de311",
      "metadata": {
        "id": "663de311"
      },
      "source": [
        "## Introduction and Goals: ##\n",
        "\n",
        "**Introduction:**\n",
        "\n",
        "The goal of this analysis is to develop and create and develop a model capable of processing text (in this case movie reviews) and interpreting and classifying it appropriately in terms of positive and negative for Film Junky Union for an effective filtering and categorisation system for movie reviews.\n",
        "\n",
        "**Goals:**\n",
        "\n",
        "The project outline is as follows:\n",
        "- Data loading and preperation, getting an overall look at the data and cheking missing/duplicate values and implement any necessary preprocessing.\n",
        "- Perform an exploratory data analysis, look at the overall distribution of reviews per movie (as in how many reviews per movie) and how the amount of movies has increased drastically since film became a popular artistic medium. Look at the distribution of ratings (1-10) and the distribution of polarities (positive and negative) for all reviews then draw conclusions.\n",
        "- Train various models keeping the company goal of an F1 score >= 0.85 in mind. Will normalise the data in 2 seperate ways for the classification models (LogisticRegression, RandomForestClassifier and LGBMClassifier) using the NLTK and spaCy methods of lemmatization respectively. Then train BERT based models interpreting results using seperate classification models (LogisticRegression, LGBMClassifier)\n",
        "- To round off model development will test each model against the test set and determine final f1 score thus determining the best performing model and the best recommendation for Film Junky Union.\n",
        "- Additionally will test each model against some select reviews and see how well the perform individually. Then conclude findings for the analysis.\n",
        "\n",
        "**NOTE:**\n",
        "f1 score is the harmonic mean of precision and recall, balances both, high precision says there are few false positives (how many the model said were correct but weren't), high recall says there are few false negatives (how many the model said were wrong but weren't). If for e.g. the model predicts mostly negatives but misses real positives accuracy can look good but f1 reveals weakness."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d308ea8e",
      "metadata": {
        "id": "d308ea8e"
      },
      "source": [
        "### Data Loading and Prep: ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "563237cb",
      "metadata": {
        "id": "563237cb"
      },
      "outputs": [],
      "source": [
        "#initialisation\n",
        "import math\n",
        "import nltk\n",
        "import os\n",
        "import re\n",
        "import spacy\n",
        "import torch\n",
        "import transformers\n",
        "from datasets import Dataset\n",
        "from google.colab import drive\n",
        "import lightgbm as lgb\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import sklearn.metrics as metrics\n",
        "from lightgbm import LGBMClassifier\n",
        "from matplotlib import pyplot as plt\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import StratifiedKFold, cross_val_score, GridSearchCV\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.preprocessing import normalize, StandardScaler\n",
        "from tqdm.auto import tqdm\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "q6k-0BQf3eMV",
      "metadata": {
        "id": "q6k-0BQf3eMV"
      },
      "outputs": [],
      "source": [
        "!wget -O imdb_reviews.tsv \"https://github.com/michaelms123/Film-Junky-Union-Analysis/raw/refs/heads/main/imdb_reviews%20(1).tsv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9d0ed3c",
      "metadata": {
        "id": "e9d0ed3c"
      },
      "outputs": [],
      "source": [
        "df_reviews = pd.read_csv('imdb_reviews.tsv', sep='\\t', dtype={'votes': 'Int64'})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28761560",
      "metadata": {
        "id": "28761560"
      },
      "outputs": [],
      "source": [
        "df_reviews.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c5e4f59",
      "metadata": {
        "id": "1c5e4f59"
      },
      "outputs": [],
      "source": [
        "df_reviews.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4dff1207",
      "metadata": {
        "id": "4dff1207"
      },
      "outputs": [],
      "source": [
        "df_reviews.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "720ff486",
      "metadata": {
        "id": "720ff486"
      },
      "outputs": [],
      "source": [
        "df_reviews.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6118206",
      "metadata": {
        "id": "d6118206"
      },
      "outputs": [],
      "source": [
        "df_reviews.duplicated().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ab5pIiPlJZC3",
      "metadata": {
        "id": "Ab5pIiPlJZC3"
      },
      "source": [
        "### Data Loading and Prep Conclusions: ###\n",
        "- Loaded the 'tsv' file (tab seperated values) the formatting for naming conventions (typically snake_case) is in order and doesn't require renaming or processing.\n",
        "- The general info and description for the data shows that the datatypes for the columns are in order and won't need conversion.\n",
        "- Check for missing/duplicate data reveals there are none and the dataset does not require further editing."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e292ba5",
      "metadata": {
        "id": "7e292ba5"
      },
      "source": [
        "## Exploratory Data Analysis: ##"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c732a148",
      "metadata": {
        "id": "c732a148"
      },
      "outputs": [],
      "source": [
        "#creating a bar graph of all movies over the years\n",
        "plt.figure(figsize=(16, 8))\n",
        "total_movies_df = df_reviews[['tconst', 'start_year']].drop_duplicates()['start_year'].value_counts().sort_index()\n",
        "total_movies_df = total_movies_df.reindex(index=np.arange(total_movies_df.index.min(), max(total_movies_df.index.max(), 2021))).fillna(0)\n",
        "total_movies_df.plot(kind='bar')\n",
        "plt.title('Number of Movies Over Years')\n",
        "plt.xlabel('Years')\n",
        "plt.ylabel('Total Movies')\n",
        "plt.xticks(rotation=90)\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27334e59",
      "metadata": {
        "id": "27334e59"
      },
      "outputs": [],
      "source": [
        "#creating bar graph that shows negative and positive reviews stacked and rolling average (every 5 yrs)\n",
        "plt.figure(figsize=(16, 8))\n",
        "total_reviews_df = df_reviews.groupby(['start_year', 'pos'])['pos'].count().unstack()\n",
        "total_reviews_df = total_reviews_df.reindex(index=np.arange(total_reviews_df.index.min(), max(total_reviews_df.index.max(), 2021))).fillna(0)\n",
        "\n",
        "ax = total_reviews_df.plot(kind='bar', stacked=True, ax=plt.gca(), label='#reviews (neg, pos)')\n",
        "\n",
        "total_reviews_df = total_reviews_df.reindex(index=np.arange(total_reviews_df.index.min(), max(total_reviews_df.index.max(), 2021))).fillna(0)\n",
        "#average no. of reviews:\n",
        "avg_reviews = (total_reviews_df.sum(axis=1).astype(float)/total_movies_df.replace(0, np.nan)).fillna(0)\n",
        "\n",
        "#line graph that shows the rolling mean over 5 years\n",
        "ax_2 = ax.twinx()\n",
        "avg_reviews.reset_index(drop=True).rolling(5).mean().plot(color='red', label='Review per Movie (Rolling Avg Over 5 Yrs)', ax=ax_2)\n",
        "\n",
        "lines, labels = ax_2.get_legend_handles_labels()\n",
        "ax.legend(lines, labels, loc='upper left')\n",
        "ax.set_title('Number of Reviews Over Years (Positive and Negative)')\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "810e23e6",
      "metadata": {
        "id": "810e23e6"
      },
      "outputs": [],
      "source": [
        "#bar graph/kde plot of reviews per movie\n",
        "fig, axs = plt.subplots(1, 2, figsize=(16, 5))\n",
        "ax = axs[0]\n",
        "reviews = df_reviews.groupby('tconst')['review'].count().value_counts().sort_index()\n",
        "reviews.plot(kind='bar', ax=ax)\n",
        "ax.set_title('Bar Graph of Review per Movie')\n",
        "\n",
        "ax2 = axs[1]\n",
        "reviews = df_reviews.groupby('tconst')['review'].count()\n",
        "sns.kdeplot(reviews, ax=ax2)\n",
        "ax2.set_title('KDE Plot of Reviews per Movie')\n",
        "\n",
        "fig.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "412927bc",
      "metadata": {
        "id": "412927bc"
      },
      "outputs": [],
      "source": [
        "#checking the amount of positive to negative reviews\n",
        "df_reviews['pos'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddeb1c36",
      "metadata": {
        "id": "ddeb1c36"
      },
      "outputs": [],
      "source": [
        "#bar graph showing the distribution of ratings (1-10) for both sets of data (training/test)\n",
        "fig, axs = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "ax = axs[0]\n",
        "ratings_train = df_reviews.query('ds_part == \"train\"')['rating'].value_counts().sort_index()\n",
        "ratings_train = ratings_train.reindex(index=np.arange(min(ratings_train.index.min(), 1), max(ratings_train.index.max(), 11))).fillna(0)\n",
        "ratings_train.plot(kind='bar', ax=ax)\n",
        "ax.set_ylim([0, 5000])\n",
        "ax.set_title('Train Set: Distribution of Ratings')\n",
        "\n",
        "ax2 = axs[1]\n",
        "ratings_test = df_reviews.query('ds_part == \"test\"')['rating'].value_counts().sort_index()\n",
        "ratings_test = ratings_test.reindex(index=np.arange(min(ratings_test.index.min(), 1), max(ratings_test.index.max(), 11))).fillna(0)\n",
        "ratings_test.plot(kind='bar', ax=ax2)\n",
        "ax2.set_ylim([0, 5000])\n",
        "ax2.set_title('The Test Set: Distribution of Ratings')\n",
        "\n",
        "fig.tight_layout()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e412ebe",
      "metadata": {
        "id": "2e412ebe"
      },
      "outputs": [],
      "source": [
        "# kde plots for negative/positive rating distributions for train/test sets\n",
        "fig, axs = plt.subplots(1, 2, figsize=(16, 5))\n",
        "\n",
        "ax = axs[0]\n",
        "neg_pos_rev_train = df_reviews.query('ds_part == \"train\"').groupby(['start_year', 'pos'])['pos'].count().unstack()\n",
        "neg_pos_rev_train = neg_pos_rev_train.reindex(index=np.arange(neg_pos_rev_train.index.min(), max(neg_pos_rev_train.index.max(), 2021))).fillna(0)\n",
        "sns.kdeplot(neg_pos_rev_train[0], color='red', label='Negative', ax=ax)\n",
        "sns.kdeplot(neg_pos_rev_train[1], color='green', label='Positive', ax=ax)\n",
        "ax.legend()\n",
        "ax.set_title('Train Set: Distribution of Review Polarities per Movie')\n",
        "\n",
        "ax2 = axs[1]\n",
        "neg_pos_rev_test = df_reviews.query('ds_part == \"test\"').groupby(['start_year', 'pos'])['pos'].count().unstack()\n",
        "neg_pos_rev_test = neg_pos_rev_test.reindex(index=np.arange(neg_pos_rev_test.index.min(), max(neg_pos_rev_test.index.max(), 2021))).fillna(0)\n",
        "sns.kdeplot(neg_pos_rev_test[0], color='red', label='Negative', ax=ax2)\n",
        "sns.kdeplot(neg_pos_rev_test[1], color='green', label='Positive', ax=ax2)\n",
        "ax2.legend()\n",
        "ax2.set_title('Test Set: Distribution of Review Polarities per Movie')\n",
        "\n",
        "fig.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aZp5nuhMOeYz",
      "metadata": {
        "id": "aZp5nuhMOeYz"
      },
      "source": [
        "### EDA Conclusions: ###\n",
        "The Exploratory Data Analysis reveals a few key things about the data and gives some helpful insight before testing:\n",
        "- The number of movies made per year has increased drastically over the years from the inception of cinema.\n",
        "- The average amount of review (per 5 years) has increased in a similar trend to movies output over time.\n",
        "- The amount of reviews per movie ranges with the most common amount being one before tapering off and spiking slightly around 30 reviews.\n",
        "- The amount of negative and positive reviews is very close to equal.\n",
        "- The range of reviews from 1-10 show a very simiilar distribution between the train/test datasets with both showing little to no reviews rated from 5-6 and the two most common for both being 1 and 10. This indicates that people rate strongly in either direction (1 being very poor, 10 being excellent) but are very rarely lukewarm in their reception to film neither liking it nor disliking it.\n",
        "- The distribution of review polarity (positive/negative) is close to equal and very similar from train to test set. With postive having a marginally higher spike than negative but being distributed over fewer values than the negative graph\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e93750ff",
      "metadata": {
        "id": "e93750ff"
      },
      "source": [
        "## Evaluation Procedure, Normalisation and Model Training: ##"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4181b16f",
      "metadata": {
        "id": "4181b16f"
      },
      "source": [
        "### Evaluation Procedure: ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9a30fcd",
      "metadata": {
        "id": "a9a30fcd"
      },
      "outputs": [],
      "source": [
        "#creating an evaluation procedure for model training (works with typical classifiers and classification models used for interpreting BERT results)\n",
        "def evaluate_model(model, train_features, train_target, test_features, test_target, tokenizer=None, is_bert=False, device='cuda'):\n",
        "    eval_stats = {}\n",
        "    fig, axs = plt.subplots(1, 3, figsize=(20, 6))\n",
        "\n",
        "    for type, features, target in (('train', train_features, train_target), ('test', test_features, test_target)):\n",
        "\n",
        "        eval_stats[type] = {}\n",
        "\n",
        "        pred_target = model.predict(features)\n",
        "        pred_proba = model.predict_proba(features)[:, 1]\n",
        "\n",
        "        #F1 Scoring:\n",
        "        f1_thresholds = np.arange(0, 1.01, 0.05)\n",
        "        f1_scores = [metrics.f1_score(target, pred_proba>=threshold) for threshold in f1_thresholds]\n",
        "\n",
        "        #ROC\n",
        "        fpr, tpr, roc_thresholds = metrics.roc_curve(target, pred_proba)\n",
        "        roc_auc = metrics.roc_auc_score(target, pred_proba)\n",
        "        eval_stats[type]['ROC AUC'] = roc_auc\n",
        "\n",
        "        #PRC\n",
        "        precision, recall, pr_thresholds = metrics.precision_recall_curve(target, pred_proba)\n",
        "        aps = metrics.average_precision_score(target, pred_proba)\n",
        "        eval_stats[type]['APS'] = aps\n",
        "\n",
        "        if type == 'train':\n",
        "            color = 'blue'\n",
        "        else:\n",
        "            color = 'green'\n",
        "\n",
        "        #F1 Score Plotting:\n",
        "        ax = axs[0]\n",
        "        max_f1_score_idx = np.argmax(f1_scores)\n",
        "        ax.plot(f1_thresholds, f1_scores, color=color, label=f'{type}, max={f1_scores[max_f1_score_idx]:.2f} @ {f1_thresholds[max_f1_score_idx]:.2f}')\n",
        "\n",
        "        for threshold in (0.2, 0.4, 0.5, 0.6, 0.8):\n",
        "            closest_value_idx = np.argmin(np.abs(f1_thresholds - threshold))\n",
        "            marker_colour = 'orange' if threshold != 0.5 else 'red'\n",
        "            ax.plot(f1_thresholds[closest_value_idx], f1_scores[closest_value_idx], color=marker_colour, marker='X', markersize=7)\n",
        "        ax.set_xlim([-0.02, 1.02])\n",
        "        ax.set_ylim([-0.02, 1.02])\n",
        "        ax.set_xlabel('Threshold')\n",
        "        ax.set_ylabel('F1')\n",
        "        ax.legend(loc='upper left')\n",
        "        ax.set_title('F1 Score')\n",
        "\n",
        "        #ROC curve Plotting\n",
        "        ax2 = axs[1]\n",
        "        ax2.plot(fpr, tpr, color=color, label=f'{type}, ROC AUC: {roc_auc:.2f}')\n",
        "        for threshold in (0.2, 0.4, 0.5, 0.6, 0.8):\n",
        "            closest_value_idx = np.argmin(np.abs(roc_thresholds - threshold))\n",
        "            marker_colour = 'orange' if threshold != 0.5 else 'red'\n",
        "            ax.plot(fpr[closest_value_idx], tpr[closest_value_idx], color=marker_colour, marker='X', markersize=7)\n",
        "        ax2.plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
        "        ax2.set_xlim([-0.02, 1.02])\n",
        "        ax2.set_ylim([-0.02, 1.02])\n",
        "        ax2.set_xlabel('FPR')\n",
        "        ax2.set_ylabel('TPR')\n",
        "        ax2.legend(loc='lower center')\n",
        "        ax2.set_title('ROC Curve')\n",
        "\n",
        "        #PRC Plotting\n",
        "        ax3 = axs[2]\n",
        "        ax3.plot(recall, precision, color=color, label=f'{type}, AP={aps:.2f}')\n",
        "        for threshold in (0.2, 0.4, 0.5, 0.6, 0.8):\n",
        "            closest_value_idx = np.argmin(np.abs(pr_thresholds - threshold))\n",
        "            marker_colour = 'orange' if threshold != 0.5 else 'red'\n",
        "            ax3.plot(recall[closest_value_idx], precision[closest_value_idx], color=marker_colour, marker='X', markersize=7)\n",
        "        ax3.set_xlim([-0.02, 1.02])\n",
        "        ax3.set_ylim([-0.02, 1.02])\n",
        "        ax3.set_xlabel('Recall')\n",
        "        ax3.set_ylabel('Precision')\n",
        "        ax3.legend(loc='upper left')\n",
        "        ax3.set_title('PRC')\n",
        "\n",
        "        eval_stats[type]['Accuracy'] = metrics.accuracy_score(target, pred_target)\n",
        "        eval_stats[type]['F1'] = metrics.f1_score(target, pred_target)\n",
        "\n",
        "    df_eval_stats = pd.DataFrame(eval_stats)\n",
        "    df_eval_stats = df_eval_stats.round(2)\n",
        "    df_eval_stats = df_eval_stats.reindex(index=('Accuracy', 'F1', 'APS', 'ROC AUC'))\n",
        "\n",
        "    print(df_eval_stats)\n",
        "\n",
        "    return"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb77fc18",
      "metadata": {
        "id": "cb77fc18"
      },
      "source": [
        "### Normalisation (NLTK and spaCy): ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f2dbb05",
      "metadata": {
        "id": "3f2dbb05"
      },
      "outputs": [],
      "source": [
        "#using NLTK\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    filtered_tokens = [token for token in tokens if token.isalpha() and token not in stop_words]\n",
        "    lemmas = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
        "    return \" \".join(lemmas)\n",
        "\n",
        "df_reviews['review_norm'] = df_reviews['review'].apply(lemmatize_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6c1b10c",
      "metadata": {
        "id": "c6c1b10c"
      },
      "outputs": [],
      "source": [
        "#using spaCy\n",
        "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
        "def spacy_lemmatize_text(text):\n",
        "    doc = nlp(text.lower())\n",
        "    lemmas_spacy = [\n",
        "        token.lemma_\n",
        "        for token in doc\n",
        "        if not token.is_punct\n",
        "        and not token.like_num\n",
        "        and token.is_alpha\n",
        "    ]\n",
        "    return \" \".join(lemmas_spacy)\n",
        "\n",
        "df_reviews['review_norm_spacy'] = df_reviews['review'].apply(spacy_lemmatize_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ce7abdc",
      "metadata": {
        "id": "8ce7abdc"
      },
      "outputs": [],
      "source": [
        "df_reviews.head(2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zRNSBm3PRvMN",
      "metadata": {
        "id": "zRNSBm3PRvMN"
      },
      "source": [
        "### Normalisation Conclusions: ###\n",
        "- Opted to normalise and preprocess the text in the 2 most popular ways, NLTK and spaCy, to determine if model performance would be affected at all by either and which would be most beneficial for model training\n",
        "- spaCy definitely has a much longer runtime but more aggresively lemmatizes the data.\n",
        "- Wrapped both options into their own function 'lemmatize_text' for NLTK and 'spacy_lemmatize_text' for spaCy.\n",
        "- Stored the lemmatized text in their own respective columns 'review_norm' and 'review_norm_spacy'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2242d9e2",
      "metadata": {
        "id": "2242d9e2"
      },
      "source": [
        "### Train, Test, Split: ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8db8f38",
      "metadata": {
        "id": "a8db8f38"
      },
      "outputs": [],
      "source": [
        "df_reviews_train = df_reviews.query('ds_part == \"train\"').copy()\n",
        "df_reviews_test = df_reviews.query('ds_part == \"test\"').copy()\n",
        "\n",
        "#NLTK features\n",
        "train_features = df_reviews_train['review_norm']\n",
        "test_features = df_reviews_test['review_norm']\n",
        "#spaCy features\n",
        "train_features_spacy = df_reviews_train['review_norm_spacy']\n",
        "test_features_spacy = df_reviews_test['review_norm_spacy']\n",
        "#targets\n",
        "train_target = df_reviews_train['pos']\n",
        "test_target = df_reviews_test['pos']\n",
        "\n",
        "print(df_reviews_train.shape)\n",
        "print(df_reviews_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f085263",
      "metadata": {
        "id": "9f085263"
      },
      "outputs": [],
      "source": [
        "#vectorizing the text\n",
        "tfidf_nltk = TfidfVectorizer(\n",
        "    max_features=5000,\n",
        "    ngram_range=(1, 2),\n",
        "    min_df=5,\n",
        "    max_df=0.9,\n",
        "    sublinear_tf=True\n",
        ")\n",
        "tfidf_spacy = TfidfVectorizer(\n",
        "    max_features=5000,\n",
        "    ngram_range=(1, 2),\n",
        "    min_df=5,\n",
        "    max_df=0.9,\n",
        "    sublinear_tf=True\n",
        ")\n",
        "#NLTK\n",
        "train_features_tfidf = tfidf_nltk.fit_transform(train_features)\n",
        "test_features_tfidf = tfidf_nltk.transform(test_features)\n",
        "#spaCy\n",
        "train_features_spacy_tfidf = tfidf_spacy.fit_transform(train_features_spacy)\n",
        "test_features_spacy_tfidf = tfidf_spacy.transform(test_features_spacy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec52294b",
      "metadata": {
        "id": "ec52294b"
      },
      "outputs": [],
      "source": [
        "df_reviews_train['pos'].value_counts(normalize=True)\n",
        "df_reviews_test['pos'].value_counts(normalize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dY8GP4izTjM7",
      "metadata": {
        "id": "dY8GP4izTjM7"
      },
      "source": [
        "### Train, Test, Split and Vectorisation Conclusions: ###\n",
        "- The data was already split within the original dataframe (ds_part == 'train', ds_part == 'test')\n",
        "- Vectorised each version of the lemmatized text (NLTK/spaCy) with tfidf vectoriser and used 2 seperate vectorisers respectively\n",
        "- Created train and test features for both NLTK and spaCy and stored them in their own columns 'train_features_tfidf', 'test_features_tfidf' for NLTK and 'train_features_spacy_tfidf', 'test_features_spacy_tfidf' for spaCy."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1245fe5a",
      "metadata": {
        "id": "1245fe5a"
      },
      "source": [
        "### Model Training (NLTK, TF-IDF pre-processing): ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8eaf85ec",
      "metadata": {
        "id": "8eaf85ec"
      },
      "source": [
        "**Logistic Regression Model (baseline):**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "063ee22f",
      "metadata": {
        "id": "063ee22f"
      },
      "outputs": [],
      "source": [
        "best_lr_model = LogisticRegression(max_iter=2000, class_weight='balanced', C=1.0, solver='liblinear', random_state=246)\n",
        "best_lr_model.fit(train_features_tfidf, train_target)\n",
        "evaluate_model(best_lr_model, train_features_tfidf, train_target, test_features_tfidf, test_target)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa8aeda3",
      "metadata": {
        "id": "fa8aeda3"
      },
      "source": [
        "**RandomForestClassifier:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73722961",
      "metadata": {
        "id": "73722961"
      },
      "outputs": [],
      "source": [
        "rf_model = RandomForestClassifier(n_estimators=300, max_depth=20, min_samples_leaf=3, min_samples_split=5, class_weight='balanced', n_jobs=-1, random_state=246)\n",
        "rf_model.fit(train_features_tfidf, train_target)\n",
        "evaluate_model(rf_model, train_features_tfidf, train_target, test_features_tfidf, test_target)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f5baa59",
      "metadata": {
        "id": "0f5baa59"
      },
      "source": [
        "**LightGBMClassifier:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fd01655",
      "metadata": {
        "id": "6fd01655"
      },
      "outputs": [],
      "source": [
        "lgbm_model = LGBMClassifier(\n",
        "    n_estimators=500,\n",
        "    learning_rate=0.05,\n",
        "    num_leaves=31,\n",
        "    random_state=246,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_alpha=1.0,\n",
        "    reg_lambda=1.0,\n",
        "    n_jobs=-1\n",
        ")\n",
        "lgbm_model.fit(train_features_tfidf, train_target)\n",
        "evaluate_model(lgbm_model, train_features_tfidf, train_target, test_features_tfidf, test_target)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d264e0a4",
      "metadata": {
        "id": "d264e0a4"
      },
      "source": [
        "### Model Training (spaCy, TF-IDF pre-processing): ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20b85e2f",
      "metadata": {
        "id": "20b85e2f"
      },
      "source": [
        "**Logistic Regression**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c850251f",
      "metadata": {
        "id": "c850251f"
      },
      "outputs": [],
      "source": [
        "lr_model_spacy = LogisticRegression(max_iter=1000, C=0.5, random_state=246)\n",
        "lr_model_spacy.fit(train_features_spacy_tfidf, train_target)\n",
        "evaluate_model(lr_model_spacy, train_features_spacy_tfidf, train_target, test_features_spacy_tfidf, test_target)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "089d4d7e",
      "metadata": {
        "id": "089d4d7e"
      },
      "source": [
        "**RandomForestClassifier:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80b5bff7",
      "metadata": {
        "id": "80b5bff7"
      },
      "outputs": [],
      "source": [
        "rf_model_spacy = RandomForestClassifier(n_estimators=200, max_depth=12, min_samples_split=3, random_state=246)\n",
        "rf_model_spacy.fit(train_features_spacy_tfidf, train_target)\n",
        "evaluate_model(rf_model_spacy, train_features_spacy_tfidf, train_target, test_features_spacy_tfidf, test_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce481995",
      "metadata": {
        "id": "ce481995"
      },
      "outputs": [],
      "source": [
        "lgbm_model_spacy = LGBMClassifier(\n",
        "    n_estimators=500,\n",
        "    learning_rate=0.05,\n",
        "    num_leaves=31,\n",
        "    random_state=246,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_alpha=1.0,\n",
        "    reg_lambda=1.0,\n",
        "    n_jobs=-1\n",
        ")\n",
        "lgbm_model_spacy.fit(train_features_spacy_tfidf, train_target)\n",
        "evaluate_model(lgbm_model_spacy, train_features_spacy_tfidf, train_target, test_features_spacy_tfidf, test_target)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "858ce851",
      "metadata": {
        "id": "858ce851"
      },
      "source": [
        "### BERT Models: ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb9a6fa9",
      "metadata": {
        "id": "bb9a6fa9"
      },
      "outputs": [],
      "source": [
        "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "config = transformers.BertConfig.from_pretrained('bert-base-uncased')\n",
        "model = transformers.BertModel.from_pretrained('bert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d522722",
      "metadata": {
        "id": "5d522722"
      },
      "outputs": [],
      "source": [
        "def BERT_text_to_embeddings(texts, tokenizer, model, max_length=512, force_device=None, disable_progress_bar=False):\n",
        "\n",
        "    ids_list = []\n",
        "    attention_mask_list = []\n",
        "\n",
        "    batch_size = 500\n",
        "\n",
        "    for text in texts:\n",
        "        ids = tokenizer.encode(text.lower(), add_special_tokens=True, truncation=True, max_length=max_length)\n",
        "\n",
        "        padded = np.array(ids + [0] * (max_length - len(ids)))\n",
        "        attention_mask = np.where(padded != 0, 1, 0)\n",
        "\n",
        "        ids_list.append(padded)\n",
        "        attention_mask_list.append(attention_mask)\n",
        "\n",
        "    if force_device is not None:\n",
        "        device = torch.device(force_device)\n",
        "    else:\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    model.to(device)\n",
        "    if not disable_progress_bar:\n",
        "        print(f'Using the {device} device.')\n",
        "\n",
        "    embeddings = []\n",
        "\n",
        "    for i in tqdm(range(math.ceil(len(ids_list)/batch_size)), disable=disable_progress_bar):\n",
        "        ids_batch = torch.LongTensor(ids_list[batch_size*i:batch_size*(i+1)]).to(device)\n",
        "        attention_mask_batch = torch.LongTensor(attention_mask_list[batch_size*i:batch_size*(i+1)]).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "            batch_embeddings = model(input_ids=ids_batch, attention_mask=attention_mask_batch)\n",
        "        embeddings.append(batch_embeddings[0][:,0,:].detach().cpu().numpy())\n",
        "\n",
        "    return np.concatenate(embeddings)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd468167",
      "metadata": {
        "id": "bd468167"
      },
      "outputs": [],
      "source": [
        "bert_train_features = BERT_text_to_embeddings(df_reviews_train['review_norm'], tokenizer, model, force_device='cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jhPTZKd_1gO7",
      "metadata": {
        "id": "jhPTZKd_1gO7"
      },
      "outputs": [],
      "source": [
        "bert_test_features = BERT_text_to_embeddings(df_reviews_test['review_norm'], tokenizer, model, force_device='cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fV8UWsAg8wRk",
      "metadata": {
        "id": "fV8UWsAg8wRk"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "bert_train_features_scaled = scaler.fit_transform(bert_train_features)\n",
        "bert_test_features_scaled = scaler.transform(bert_test_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3961ec6",
      "metadata": {
        "id": "d3961ec6"
      },
      "outputs": [],
      "source": [
        "print(df_reviews_train['review_norm'].shape)\n",
        "print(bert_train_features.shape)\n",
        "print(train_target.shape)\n",
        "\n",
        "print(df_reviews_test['review_norm'].shape)\n",
        "print(bert_test_features.shape)\n",
        "print(test_target.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vQxFOhqtyfze",
      "metadata": {
        "id": "vQxFOhqtyfze"
      },
      "outputs": [],
      "source": [
        "bert_lr = LogisticRegression(max_iter=3000, C=10, random_state=246)\n",
        "bert_lr.fit(bert_train_features_scaled, train_target)\n",
        "lr_pred_proba = bert_lr.predict_proba(bert_test_features)[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VAeSYz5g3TPh",
      "metadata": {
        "id": "VAeSYz5g3TPh"
      },
      "outputs": [],
      "source": [
        "evaluate_model(bert_lr, bert_train_features_scaled, train_target, bert_test_features_scaled, test_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6O9Ht-SARMJ1",
      "metadata": {
        "id": "6O9Ht-SARMJ1"
      },
      "outputs": [],
      "source": [
        "bert_lgbm = LGBMClassifier(\n",
        "    num_leaves=31,\n",
        "    learning_rate=0.02,\n",
        "    n_estimators=1000,\n",
        "    min_child_samples=150,\n",
        "    reg_alpha=1.0,\n",
        "    reg_lambda=5.0,\n",
        "    subsample=0.8,\n",
        "    subsample_freq=1,\n",
        "    colsample_bytree=0.8,\n",
        "    max_depth=4,\n",
        "    random_state=246,\n",
        "    verbosity=-1\n",
        ")\n",
        "bert_lgbm.fit(bert_train_features, train_target)\n",
        "pred_proba = bert_lgbm.predict_proba(bert_test_features)[:, 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yHmLdm-mnnU_",
      "metadata": {
        "id": "yHmLdm-mnnU_"
      },
      "outputs": [],
      "source": [
        "evaluate_model(bert_lgbm, bert_train_features, train_target, bert_test_features, test_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "294d7055",
      "metadata": {
        "id": "294d7055"
      },
      "outputs": [],
      "source": [
        "models = {\n",
        "    \"Logistic Regression (NLTK)\": best_lr_model,\n",
        "    \"Random Forest (NLTK)\": rf_model,\n",
        "    \"LGBM (NLTK)\": lgbm_model,\n",
        "    \"Logistic Regression (spaCy)\": lr_model_spacy,\n",
        "    \"Random Forest (spaCy)\": rf_model_spacy,\n",
        "    \"LGBM (spaCy)\": lgbm_model_spacy,\n",
        "    \"BERT Logistic Regression\": bert_lr,\n",
        "    \"BERT LGBM\": bert_lgbm,\n",
        "}\n",
        "\n",
        "def test_models(train_features, test_features, target_train, target_test, label):\n",
        "    print(f'\\n=== {label} Results ===')\n",
        "\n",
        "    if hasattr(train_features, 'toarray'):\n",
        "        train_features = train_features.toarray()\n",
        "    if hasattr(test_features, 'toarray'):\n",
        "        test_features = test_features.toarray()\n",
        "\n",
        "    if 'NLTK' in label:\n",
        "        model_type = 'NLTK'\n",
        "    elif 'spaCy' in label:\n",
        "        model_type = 'spaCy'\n",
        "    elif 'BERT' in label:\n",
        "        model_type = 'BERT'\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown label type: {label}\")\n",
        "\n",
        "    filtered_models = {\n",
        "        name: mmodel\n",
        "        for name, mmodel in models.items()\n",
        "        if model_type in name\n",
        "    }\n",
        "\n",
        "    for name, mmodel in filtered_models.items():\n",
        "\n",
        "        pred_test = mmodel.predict(test_features)\n",
        "\n",
        "        if hasattr(mmodel, 'predict_proba'):\n",
        "            pred_proba = mmodel.predict_proba(test_features)[:, 1]\n",
        "        else:\n",
        "            pred_proba = pred_test\n",
        "\n",
        "\n",
        "        accuracy = metrics.accuracy_score(test_target, pred_test)\n",
        "        f1_score = metrics.f1_score(test_target, pred_test)\n",
        "        roc = metrics.roc_auc_score(test_target, pred_proba)\n",
        "        print(f'\\n{name}:')\n",
        "        print(f'Accuracy: {accuracy:.2f}')\n",
        "        print(f'F1 Score: {f1_score:.2f}')\n",
        "        print(f'ROC-AUC Score: {roc:.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3e4314c",
      "metadata": {
        "id": "b3e4314c"
      },
      "outputs": [],
      "source": [
        "test_models(train_features_tfidf, test_features_tfidf, train_target, test_target, \"NLTK PreProcessing\")\n",
        "test_models(train_features_spacy_tfidf, test_features_spacy_tfidf, train_target, test_target, \"spaCy PreProcessing\")\n",
        "test_models(bert_train_features_scaled, bert_test_features_scaled, train_target, test_target, 'BERT Classifiers')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LJgXnS2MMkFO",
      "metadata": {
        "id": "LJgXnS2MMkFO"
      },
      "source": [
        "### BERT Fine Tuned Final Testing: ###"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Jo9nc3jWVBNf",
      "metadata": {
        "id": "Jo9nc3jWVBNf"
      },
      "source": [
        "### Model Training and Testing Conclusions: ###\n",
        "Trained a wide variety of models to determine the best approach, started off with regular classifiers, LogisticRegression and RandomForestClassifier before trying a gradient boosting model LGBM. Then trained a fixed BERT model that can't classify on its own and needs assistance from other classifiers (Here Logistic Regression and LGBM), finally trained a BERT model that can classify results automatically and does not need to have the features extracted with an external classifier.\n",
        "\n",
        "Evaluated all models with the exception of the fine tuned BERT model with the evaluate model function, which produces graphs for F1, PRC (Precision Recall Curve), ROC-AUC (Receiver Operating Characteristic Area Under Curve) as well as produce predictions of the trained model on the test set\n",
        "\n",
        "**Logistic Regression (NLTK/spaCy)**\n",
        "\n",
        "- Used this model as a baseline as it is simple and requires little hyperparater tuning.\n",
        "- Performed well on training set with f1 score being aroung 0.9 for both, and they performed very well on the test set with the NLTK model scoring 0.88 and the spaCy model scoring around 0.89 well above the threshold\n",
        "- This indicates minimal overfitting and although all company requirements are met will need to perform further testing to determine if this is the optimal model.\n",
        "\n",
        "**RandomForestClassifier (NLTK/spaCy)**\n",
        "\n",
        "- Again performed well on the training set but marginally worse for both when compared to the test data.\n",
        "- This model does not have serious evidence of overfitting between training and testing and the NLTK model scored a final F1 around 0.84 and the spaCy model around 0.83 so fairly accurate all things considered.\n",
        "- However the slow runtime and the fact that it falls below the threshold can rule the model out in the final recommendation.\n",
        "\n",
        "**LightGBMClassifier (NLTK/spaCy)**\n",
        "\n",
        "- These models scored the second best across both styles of vectorisation with final F1 score of 0.87 (NLTK) and 0.88 (spaCy) respectively\n",
        "- Marginally less accurate and a fair amount slower than the Logistic Regression model which has performed the best thus far.\n",
        "- Can likely rule this model out in the final analysis due to the slightly less accurate but definitely slower runtime, but will train a few BERT based models (specifically for Natural Language Representation) to determine if the Logistic Regression model is still the best option.\n",
        "\n",
        "**BERT (Logistic Regression/LGBM)**\n",
        "\n",
        "- Created a fixed BERT model that was pretrained on lowercase English texts and used BERT_text_to_embeddings function to extract the embeddings for BERT, this is a drastically slower process as the BERT model takes much longer to craft embeddings than using a vectoriser in conjunction with NLTK, spaCy formatting.\n",
        "- This format works well but need to use an external classifier (LogisticRegression/LGBM) to extract features and gain predictions\n",
        "- Both scored well with a final F1 score around 0.80-0.82 the Logistic Regression model showed little evidence of overfitting but the LGBM model shows some moderate signs of overfitting with the model scoring around 0.88 on the training data but only around 0.801 F1 when compared against the test set.\n",
        "- This is a great method however it scored worse overall than the Logistic Regression model trained on both NLTK and spaCy. The long compusational time (>20 minutes) means that although it is more accurate instantly the failure to outperform the Logistic Regression model means that it can't be recommended in the final analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "213505c5",
      "metadata": {
        "id": "213505c5"
      },
      "source": [
        "### Classifying Reviews: ###"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b9f1d72",
      "metadata": {
        "id": "8b9f1d72"
      },
      "outputs": [],
      "source": [
        "my_reviews = pd.DataFrame([\n",
        "    'I did not simply like it, not my kind of movie.',\n",
        "    'Well, I was bored and felt asleep in the middle of the movie.',\n",
        "    'I was really fascinated with the movie',\n",
        "    'Even the actors looked really old and disinterested, and they got paid to be in the movie. What a soulless cash grab.',\n",
        "    'I didn\\'t expect the reboot to be so good! Writers really cared about the source material',\n",
        "    'The movie had its upsides and downsides, but I feel like overall it\\'s a decent flick. I could see myself going to see it again.',\n",
        "    'What a rotten attempt at a comedy. Not a single joke lands, everyone acts annoying and loud, even kids won\\'t like this!',\n",
        "    'Launching on Netflix was a brave move & I really appreciate being able to binge on episode after episode, of this exciting intelligent new drama.'\n",
        "], columns=['review'])\n",
        "\n",
        "my_reviews['review_norm'] = my_reviews['review'].apply(lemmatize_text)\n",
        "my_reviews['review_norm_spacy'] = my_reviews['review'].apply(spacy_lemmatize_text)\n",
        "my_reviews.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91e53e92",
      "metadata": {
        "id": "91e53e92"
      },
      "outputs": [],
      "source": [
        "nltk_texts = my_reviews['review_norm']\n",
        "spacy_texts = my_reviews['review_norm_spacy']\n",
        "bert_features = BERT_text_to_embeddings(my_reviews['review'], tokenizer, model, force_device='cuda', disable_progress_bar=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c0969a8",
      "metadata": {
        "id": "6c0969a8"
      },
      "outputs": [],
      "source": [
        "for name, model3 in models.items():\n",
        "\n",
        "    print(f'\\n ===Predictions for {name} ===')\n",
        "\n",
        "    if 'NLTK' in name:\n",
        "        X = tfidf_nltk.transform(nltk_texts)\n",
        "        probs = model3.predict_proba(X)[:, 1]\n",
        "    elif 'spaCy' in name:\n",
        "        X = tfidf_spacy.transform(spacy_texts)\n",
        "        probs = model3.predict_proba(X)[:, 1]\n",
        "    elif name in ['BERT Logistic Regression', 'BERT LGBM']:\n",
        "        probs = model3.predict_proba(bert_features)[:, 1]\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model type: {name}\")\n",
        "\n",
        "    for prob, review in zip(probs, my_reviews['review']):\n",
        "        sentiment = \"Positive\" if prob > 0.5 else 'Negative'\n",
        "        print(f'{prob:.2f}: {review}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0zdTRBC6tRUG",
      "metadata": {
        "id": "0zdTRBC6tRUG"
      },
      "source": [
        "### My Reviews Conclusions: ###\n",
        "- Tested each respective model on a small handful of reviews to see if they can classify positive and negative reviews on an individual scale.\n",
        "- The regular classifiers performed well with the Logistic Regression doing the best out of the three.\n",
        "-  The BERT based models performed well but as discussed in the testing stage they failed to outperform the Logistic Regression/LGBM models trained with NLTK/spaCy processed data and did marginally worse on an individual review basis as well as the final F1 score."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37ILk9Kxu557",
      "metadata": {
        "id": "37ILk9Kxu557"
      },
      "source": [
        "## Final Conclusions: ##\n",
        "Out of all the models tested the Logistic Regression model trained on the spaCy preprocessed text performed the best with a final F1 score of 0.89 and accurate predictions at an individual level. This is what would be best for Film Junky Union review classification system.\n",
        "\n",
        "The model:\n",
        "\n",
        "- Performs the best in terms of F1 score and exceeds the company threshold of 0.85\n",
        "- The model does not overfit from the training set to the test set.\n",
        "- Performs very well on an individual level and can classify reviews accurately and according to the tonality and language used by the reviewer.\n",
        "- Is very fast computationally and does not require a powerful GPU to run of all the models tested it is by far the most time efficient in addition to the accuracy of the model."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.23"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}